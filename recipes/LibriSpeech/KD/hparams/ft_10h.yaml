# ################################
# Model: CTC ASR on TIMIT with CRDNN (with LiGRU)
# Additions: TimeDomainSpecAugment
# Authors: Mirco Ravanelli & Peter Plantinga 2020
# ################################

# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1986
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/wav2vec-baseline/<seed>
csv_folder: /share/data/speech/jjery2243542/data/LibriSpeech/w2v_splits/char_csv
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt
model_folder: /share/data/speech/jjery2243542/result/libri/kd/kmeans_cls_sep_mul_layers/save/CKPT+2021-09-08+01-39-04+00

# Data files
data_folder: !PLACEHOLDER  # e.g. /path/to/TIMIT
train_splits: ["train-clean-100", "train-other-500"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
skip_prep: True # Skip data preparation
sample_subsets: True # Skip data preparation
wav2vec_hub: "facebook/wav2vec2-large-lv60"
#wav2vec_hub: "https://dl.fbaipublicfiles.com/fairseq/wav2vec/libri960_big.pt"
train_csv: !ref <csv_folder>/train-10h.csv
valid_csv: !ref <csv_folder>/dev-1h.csv
test_csv:
    - !ref <csv_folder>/dev-clean.csv
    - !ref <csv_folder>/dev-other.csv

# Training parameters
number_of_epochs: 15
save_ckpt_every_n_epochs: 1
batch_size: 8
gradient_accumulation: 1
lr: 0.0001
final_lr_scale: 0.01
n_warmup_steps: 600
n_hold_steps: 3000
n_decay_steps: 1000
sorting: random # choose between ascending, descending and random
load_pretrain: True

# Feature parameters
sample_rate: 16000
hidden_size: 1024

# Outputs
output_neurons: 29 # 28 char + 1 blank
blank_index: 0

# Model parameters
num_layers: 6
d_ffn: 3072
d_model: 768
model_apply_spec_augment: True
model_mask_time_prob: 0.075
model_mask_feature_prob: 0.5
model_type: transformer
dropout: 0.1
freeze_conv: True

# Dataloader options
train_dataloader_opts:
    batch_size: !ref <batch_size>

valid_dataloader_opts:
    batch_size: !ref <batch_size>

test_dataloader_opts:
    batch_size: !ref <batch_size>

#normalize: !new:speechbrain.processing.features.InputNormalization
#    norm_type: global
#
#augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment
#    sample_rate: !ref <sample_rate>
#    speeds: [95, 100, 105]
#
## Can be removed to improve speed
#env_corrupt: !new:speechbrain.lobes.augment.EnvCorrupt
#    openrir_folder: !ref <open_rir_folder>
#    babble_prob: 0.0
#    reverb_prob: 0.0
#    noise_prob: 1.0
#    noise_snr_low: 0
#    noise_snr_high: 15

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

model: !new:speechbrain.lobes.models.ConvTF.ConvTF
    source: !ref <wav2vec_hub>
    save_path: !ref /share/data/speech/jjery2243542/wav2vec_checkpoint/huggingface/
    num_layers: !ref <num_layers>
    d_ffn: !ref <d_ffn>
    d_model: !ref <d_model>
    dropout: !ref <dropout>
    model_type: !ref <model_type>
    apply_spec_augment: !ref <model_apply_spec_augment>
    mask_time_prob: !ref <model_mask_time_prob>
    mask_feature_prob: !ref <model_mask_feature_prob>
    freeze_conv: !ref <freeze_conv>

linear: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <d_model>
    n_neurons: !ref <output_neurons>
    bias: True

log_softmax: !new:speechbrain.nnet.activations.Softmax
    apply_log: True

opt_class: !name:torch.optim.Adam
    lr: !ref <lr>

lr_annealing: !new:speechbrain.nnet.schedulers.TriStageScheduler
    lr_initial: !ref <lr>
    n_warmup_steps: !ref <n_warmup_steps>
    n_hold_steps: !ref <n_hold_steps>
    n_decay_steps: !ref <n_decay_steps>
    final_lr_scale: !ref <final_lr_scale>

modules:
    model: !ref <model>
    linear: !ref <linear>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        linear: !ref <linear>
        scheduler: !ref <lr_annealing>
        counter: !ref <epoch_counter>

compute_cost: !name:speechbrain.nnet.losses.ctc_loss
    blank_index: !ref <blank_index>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

ctc_stats: !name:speechbrain.utils.metric_stats.MetricStats
    metric: !name:speechbrain.nnet.losses.ctc_loss
        blank_index: !ref <blank_index>
        reduction: batch

cer_stats: !name:speechbrain.utils.metric_stats.ErrorRateStats
wer_stats: !name:speechbrain.utils.metric_stats.ErrorRateStats

pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
    collect_in: !ref <model_folder>
    loadables:
        mdl: !ref <model>
    paths:
        mdl: !ref <model_folder>/model.ckpt
