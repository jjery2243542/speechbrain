# ################################
# Model: CTC ASR on TIMIT with CRDNN (with LiGRU)
# Additions: TimeDomainSpecAugment
# Authors: Mirco Ravanelli & Peter Plantinga 2020
# ################################

# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1986
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/wav2vec-baseline/<seed>
csv_folder: /share/data/speech/jjery2243542/data/LibriSpeech/w2v_splits/frame_csv
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

# Data files
data_folder: !PLACEHOLDER  # e.g. /path/to/TIMIT
train_splits: ["train-clean-100", "train-other-500"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
skip_prep: True # Skip data preparation
sample_subsets: True # Skip data preparation
wav2vec_hub: "facebook/wav2vec2-large-lv60"
#wav2vec_hub: "https://dl.fbaipublicfiles.com/fairseq/wav2vec/libri960_big.pt"
train_csv: !ref <csv_folder>/train-10h.csv
valid_csv: !ref <csv_folder>/dev-1h.csv
test_csv:
    - !ref <csv_folder>/dev-clean.csv
    - !ref <csv_folder>/dev-other.csv

# Training parameters
number_of_epochs: 1
save_ckpt_every_n_epochs: 1
batch_size: 8
sorting: random # choose between ascending, descending and random

# Feature parameters
sample_rate: 16000
hidden_size: 1024
proj_size: 256

# Outputs
n_phns: 42 # 39 phn + 3 non-phn
skip_tokens: ["sp", "spn", "sil"]

# W2V2 Model parameters
return_nth_layers: [3, 6, 9, 12, 15, 18]
re_init_from_nth_layer: null
freeze: True
freeze_until_nth_layer: null
add_n_layers: 0
w2v_apply_spec_augment: False
w2v_mask_time_prob: 0.075
w2v_mask_feature_prob: 0.5
use_coef: False
temperature: 1.0
eval_mode: True

# kmeans parameters
n_clusters: 500
kmeans_batch_size: 10000
random_state: 0
n_models: 6

# Dataloader options
train_dataloader_opts:
    batch_size: !ref <batch_size>

valid_dataloader_opts:
    batch_size: !ref <batch_size>

test_dataloader_opts:
    batch_size: !ref <batch_size>

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

wav2vec: !new:speechbrain.lobes.models.huggingface_wav2vec.HuggingFaceWav2Vec2MultiLayer
    source: !ref <wav2vec_hub>
    freeze: !ref <freeze>
    freeze_until_nth_layer: !ref <freeze_until_nth_layer>
    apply_spec_augment: !ref <w2v_apply_spec_augment>
    return_nth_layers: !ref <return_nth_layers>
    re_init_from_nth_layer: !ref <re_init_from_nth_layer>
    add_n_layers: !ref <add_n_layers>
    use_coef: !ref <use_coef>
    temperature: !ref <temperature>
    save_path: !ref /share/data/speech/jjery2243542/wav2vec_checkpoint/huggingface/
    mask_time_prob: !ref <w2v_mask_time_prob>
    mask_feature_prob: !ref <w2v_mask_feature_prob>
    eval_mode: !ref <eval_mode>

kmeans_models: !new:speechbrain.lobes.models.cluster.KMeansModels
    n_clusters: !ref <n_clusters>
    batch_size: !ref <kmeans_batch_size>
    random_state: !ref <random_state>
    n_models: !ref <n_models>

linear: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <hidden_size>
    n_neurons: !ref <proj_size>
    bias: True

modules:
    wav2vec: !ref <wav2vec>
    linear: !ref <linear>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        wav2vec: !ref <wav2vec>
        linear: !ref <linear>
        counter: !ref <epoch_counter>
        kmeans_models: !ref <kmeans_models>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

confusion_stats: !name:speechbrain.utils.Confusion.ConfusionStats
    n_clusters: !ref <n_clusters>
    n_labels: !ref <n_phns>
