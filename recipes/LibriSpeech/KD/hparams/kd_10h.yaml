# ################################
# Model: CTC ASR on TIMIT with CRDNN (with LiGRU)
# Additions: TimeDomainSpecAugment
# Authors: Mirco Ravanelli & Peter Plantinga 2020
# ################################

# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1986
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/wav2vec-baseline/<seed>
csv_folder: /share/data/speech/jjery2243542/data/LibriSpeech/w2v_splits/char_csv
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt
kmeans_model_folder: /share/data/speech/jjery2243542/result/libri/kd/kmeans_sep_mul_layers/save/CKPT+2021-09-05+12-14-07+00

# Data files
data_folder: !PLACEHOLDER  # e.g. /path/to/TIMIT
train_splits: ["train-clean-100", "train-other-500"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
skip_prep: True # Skip data preparation
sample_subsets: True # Skip data preparation
wav2vec_hub: "facebook/wav2vec2-large-lv60"
#wav2vec_hub: "https://dl.fbaipublicfiles.com/fairseq/wav2vec/libri960_big.pt"
train_csv: !ref <csv_folder>/train-10h.csv
valid_csv: !ref <csv_folder>/dev-1h.csv
test_csv:
    - !ref <csv_folder>/dev-clean.csv
    - !ref <csv_folder>/dev-other.csv

# Training parameters
number_of_epochs: 100
save_ckpt_every_n_epochs: 1
batch_size: 8
kmeans_batch_size: 10000
gradient_accumulation: 1
lr: 0.0001
final_lr_scale: 0.01
n_warmup_steps: 5000
n_hold_steps: 20000
n_decay_steps: 10000
sorting: random # choose between ascending, descending and random
label_smoothing: 0.0
n_models: 6
random_state: 0
soft_dist: True
gamma: 5.0

# Feature parameters
sample_rate: 16000
hidden_size: 1024
proj_size: 256

# Outputs
output_neurons: 500
blank_index: 0

# W2V2 Model parameters
return_nth_layers: [3, 6, 9, 12, 15, 18]
re_init_from_nth_layer: null
freeze: True
freeze_until_nth_layer: null
add_n_layers: 0
w2v_apply_spec_augment: False
w2v_mask_time_prob: 0.075
w2v_mask_feature_prob: 0.5
use_coef: False
temperature: 1.0
eval_mode: True

# Model parameters
num_layers: 6
d_ffn: 3072
d_model: 768
model_apply_spec_augment: True
model_mask_time_prob: 0.075
model_mask_feature_prob: 0.5
model_type: transformer
dropout: 0.1
freeze_conv: False

# Dataloader options
train_dataloader_opts:
    batch_size: !ref <batch_size>

valid_dataloader_opts:
    batch_size: !ref <batch_size>

test_dataloader_opts:
    batch_size: !ref <batch_size>

#normalize: !new:speechbrain.processing.features.InputNormalization
#    norm_type: global
#
#augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment
#    sample_rate: !ref <sample_rate>
#    speeds: [95, 100, 105]
#
## Can be removed to improve speed
#env_corrupt: !new:speechbrain.lobes.augment.EnvCorrupt
#    openrir_folder: !ref <open_rir_folder>
#    babble_prob: 0.0
#    reverb_prob: 0.0
#    noise_prob: 1.0
#    noise_snr_low: 0
#    noise_snr_high: 15

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

wav2vec: !new:speechbrain.lobes.models.huggingface_wav2vec.HuggingFaceWav2Vec2MultiLayer
    source: !ref <wav2vec_hub>
    freeze: !ref <freeze>
    freeze_until_nth_layer: !ref <freeze_until_nth_layer>
    apply_spec_augment: !ref <w2v_apply_spec_augment>
    return_nth_layers: !ref <return_nth_layers>
    re_init_from_nth_layer: !ref <re_init_from_nth_layer>
    add_n_layers: !ref <add_n_layers>
    use_coef: !ref <use_coef>
    temperature: !ref <temperature>
    save_path: !ref /share/data/speech/jjery2243542/wav2vec_checkpoint/huggingface/
    mask_time_prob: !ref <w2v_mask_time_prob>
    mask_feature_prob: !ref <w2v_mask_feature_prob>
    eval_mode: !ref <eval_mode>

model: !new:speechbrain.lobes.models.ConvTF.ConvTF
    source: !ref <wav2vec_hub>
    save_path: !ref /share/data/speech/jjery2243542/wav2vec_checkpoint/huggingface/
    num_layers: !ref <num_layers>
    d_ffn: !ref <d_ffn>
    d_model: !ref <d_model>
    dropout: !ref <dropout>
    model_type: !ref <model_type>
    apply_spec_augment: !ref <model_apply_spec_augment>
    mask_time_prob: !ref <model_mask_time_prob>
    mask_feature_prob: !ref <model_mask_feature_prob>
    freeze_conv: !ref <freeze_conv>

#conv: !name:speechbrain.nnet.CNN.Conv1d
#    in_channels: !ref <d_model>
#    out_channels: !ref <d_model>
#    kernel_size: !ref <kernel_size>

linear: !name:speechbrain.nnet.linear.Linear
    input_size: !ref <d_model>
    n_neurons: !ref <output_neurons>
    bias: True

log_softmax: !new:speechbrain.nnet.activations.Softmax
    apply_log: True

opt_class: !name:torch.optim.Adam
    lr: !ref <lr>

lr_annealing: !new:speechbrain.nnet.schedulers.TriStageScheduler
    lr_initial: !ref <lr>
    n_warmup_steps: !ref <n_warmup_steps>
    n_hold_steps: !ref <n_hold_steps>
    n_decay_steps: !ref <n_decay_steps>
    final_lr_scale: !ref <final_lr_scale>

modules:
    wav2vec: !ref <wav2vec>
    model: !ref <model>
    projection: !ref <projection>
#    linear: !ref <linear>
#    normalize: !ref <normalize>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        projection: !ref <projection>
#        linear: !ref <linear>
#        normalize: !ref <normalize>
        scheduler: !ref <lr_annealing>
        counter: !ref <epoch_counter>

nll_cost: !name:speechbrain.nnet.losses.nll_loss
    label_smoothing: !ref <label_smoothing>

kl_cost: !name:speechbrain.nnet.losses.kl_loss

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

acc_stats: !name:speechbrain.utils.Accuracy.AccuracyStats

nll_loss_stats: !name:speechbrain.utils.metric_stats.MetricStats
    metric: !name:speechbrain.nnet.losses.nll_loss
        reduction: batch

kl_loss_stats: !name:speechbrain.utils.metric_stats.MetricStats
    metric: !name:speechbrain.nnet.losses.kl_loss
        reduction: batch

kmeans_models: !new:speechbrain.lobes.models.cluster.KMeansModels
    n_clusters: !ref <output_neurons>
    batch_size: !ref <kmeans_batch_size>
    random_state: !ref <random_state>
    n_models: !ref <n_models>


projection: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <hidden_size>
    n_neurons: !ref <proj_size>
    bias: True

pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
    collect_in: !ref <kmeans_model_folder>
    loadables:
        km: !ref <kmeans_models>
        pj: !ref <projection>
    paths:
        km: !ref <kmeans_model_folder>/kmeans_models.ckpt
        pj: !ref <kmeans_model_folder>/linear.ckpt
